import util
from state import State
from node import Node
import sys


def successorRelaxed(state, action):
    ''' Return the sucessor state generated by executing `action` in `state`. '''
    return State(action.pos_effect).union(state)

def layerGoals(state, predicates):
    return State(state).union(predicates)

def goal_test(state, goal):
    ''' Return true if `state` is a goal state. '''
    return State(state).intersect(goal) == State(goal)

def h_naive(state, planning, ground):
    return 0

def h_add(state, planning, ground):
    h = dict() 
    goal = planning.goal
    X = state
    for x in X:
        h[x] = 0
    change = True
    while change:
        change = False
        actionsApplicable = ground.applicableActions(X)
        for a in actionsApplicable:
            X = successorRelaxed(X,a) #added positive effects of a
            for p in a.pos_effect:
                prev = h.get(p,sys.maxsize)

                h[p] = min(prev,(1+sum([h.get(pre, sys.maxsize) for pre in a.precond  ])))
               
                #h[p] = min(prev,(1+sum(h.get(pre, sys.maxsize) for pre in a.precond)))
                if prev != h[p]:
                    change = True
    return sum(h.get(i,sys.maxsize) for i in goal)
    
def h_max(state, planning, ground):
    h = dict() 
    #actions = planning._all_actions
    goal = planning.goal
    X = state
    for x in X:
        h[x] = 0
    change = True
    while change:
        change = False
        actionsApplicable = ground.applicableActions(X)
        for a in actionsApplicable:
            X = successorRelaxed(X,a) #added positive effects of a
            for p in a.pos_effect:
                prev = h.get(p,float('inf'))
                h[p] = min(prev,(1+max(h.get(pre, float('inf')) for pre in a.precond)))
                if prev != h[p]:
                    change = True
    return max(h.get(i,float('inf')) for i in goal)

# def h_ff(state, planning):
#     graphplan = dict() #graphplan relaxed
#     actions = planning._all_actions
#     goal = planning._problem.goal
#     X = state
#     isGoal = False
#     if X.intersect(goal) == goal: #ja estamos na meta entao o compimento (a quantidade) de acoes necessaria eh zero
#         return 0
#     level = 0
#     graphplan[(level,'state')] = X
#     #PHASE 1 - expand graph
#     while not isGoal:
#         actionsApplicable = applicable(X,actions)
#         level += 1
#         for a in actionsApplicable:
#             X = successorRelaxed(X,a) #added positive effects of a
#             if X.intersect(goal) == goal:
#                 isGoal = True
#                 break
#         graphplan[(level,'state')] = X
#         graphplan[(level,'action')] = actionsApplicable
#     #PHASE 2 - busca regressiva - partindo dos atomos do goal ate termos os atomos do state
#     thisLevelGoals = set()
#     thisLevelGoals = thisLevelGoals.union(goal)
#     relaxedActions = set()
#     while (level > 0):
#         prevLevelGoals = set()
#         for tg in thisLevelGoals:
#             if tg in graphplan[level-1,'state']:
#                 prevLevelGoals.add(tg)
#             else:
#                 for a in graphplan[level,'action']:
#                     if tg in a.pos_effect:
#                         prevLevelGoals = prevLevelGoals.union(a.precond)
#                         relaxedActions.add(a)
#                         break 
#         level -= 1
#         thisLevelGoals = prevLevelGoals.copy()
#     return (len(relaxedActions))